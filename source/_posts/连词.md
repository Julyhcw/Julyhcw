title: 论文阅读 || 基于强化学习的连词来增强神经网络用于自然语言推理  
categories: nlp Paper
mathjax: true
tags: 
   - nlp Paper
---
  
### 前言  
这篇Paper是我最近读的与其它Paper具有较大不同的地方，大多数的Paper都是在对网络结构不断的捣鼓，很少会对自然语言本身的语言学角度来创新，而这篇Paper就利用了一丢丢的语言学知识来构建模型，它提出人们通常会使用一些连词例如所以、但是等来表示两个句子之间的逻辑关系，这些词与句子的含义会有很深的联系，因此可以利用这些连词来构建模型。他们通过强化学习的方式来构建一个编码器，然后利用这个编码器来建模NLI的假设和前提，从而增强输入的表述。    

- TITLE: Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference   
- URL: http://www.aclweb.org/anthology/P18-1091  
<!-- more -->
### 方法  
#### 关于Discourse Marker（连词）  
Paper中关于这个任务称为DMP，给定一个句子（$S_1,S_2$),它原本是一个完整句子的前半部分和后半部分，模型通过这两个句子来预测它属于哪一类的连词。   
主体的框架图如下所示：  
![](http://pbcgmnu5b.bkt.clouddn.com/nlp-080500.jpg)
#### 句子编码模型  
关于DMP的句子编码，此Paper利用BookCorpus作为训练数据，它是一篇未出版的小说，数据集足够大，可以避免某些特定领域或者应用的Bias，通过预处理将其分成（$S_1,S_2,m$),其中m代表连词的标签。  
#### 关于Sentence Representation  
根据框架图，可以很直观的看到模型如何构建，其中通过BiLSTM的公式如下：  
$$\overrightarrow{r_t} = Max_{dim}([\overrightarrow{h_t^1};\overrightarrow{h_t^2};\cdots ; \overrightarrow{h_t^{|s_t|}}])$$  
$$\overrightarrow{r_t} = Max_{dim}([\overleftarrow{h_t^1};\overleftarrow{h_t^2};\cdots ; \overleftarrow{h_t^{|s_t|}}])$$  
其中Maxdim表示max pooling是在向量拼接后在每一个维度上进行的，然后将两个方向的最后的一个Hidden state以及上文经过max pooling的结果结合在一起，公式如下：  
$$r_t = [\overrightarrow{r_t};\overleftarrow{r_t};\overrightarrow{h_t^{|s_t|}};\overleftarrow{h_t^1}]$$  
其中$r_t$是句子$s_t$的向量表示，最后通过一个线性操作将其结合作为最后分类的输入：  
$$r = [r_1;r_2;r_1+r_2;r_1\odot r_2]$$  
其中$\odot$ 表示元素相乘。  
#### 小结  
这一部分就是训练连词的强化学习部分，训练好的模型会用在NLI模型中，对假设和前提进行编码。

#### NLI部分  
这一部分和其它的很多网络结构区别不大，首先对假设和前提进行编码，然后通过交互层也就是Attention机制，最后做分类，而这篇Paper最大不同就是利用上面训练好的模型对假设和前提多进行一次编码，具体和上面DMP完全一样，将最后的表示和NLI的通过BiLSTM的表示连接起来一起通过Attention做个交互，最后进行分类。具体细节可以参考论文本身。

### 总结  
这篇Paper的模式跟篇章关系识别的一致，通过连接词来判断两个句子的关系，篇章关系识别主要是通过连接词的来判断，而NLI两个句子是没有连接词的，通过训练一种通用的模型来增强NLI模型的输入的表述，也算是一种比较新颖的做法吧。这一个月看的Paper都是大同小异，全是各种神经网络加Attention，看的脑壳疼，NLP的最后肯定还是的结合语言学的知识才能有更大的突破。
