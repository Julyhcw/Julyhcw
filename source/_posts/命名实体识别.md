title: 基于BiLSTM-CRF的中文命名实体识别
categories: nlp
tags: 
   - nlp
---

### 前言  
命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。命名实体识别是信息提取、问答系统、句法分析、机器翻译、面向Semantic Web的元数据标注等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。  

<!-- more -->
  
命名实体识别在中文中进行相对于其他语言会更加困难,难点主要表现为:  
1) 命名实体类型多样,数量众多,不断有新的命名实体涌现,如新的人名、地名等,难以建立大而全的姓氏库、名字库、地址库等数据库。  
2) 命名实体构成结构比较复杂,并且某些类型的命名实体词的长度没有一定的限制,不同的实体有不同的结构,比如组织名存在大量的嵌套、别名、缩略词等问题,没有严格的规律可以遵循;人名中也存在比较长的少数民族人名或翻译过来的外国人名,没有统一的构词规范。因此,对这类命名实体识别的召回率相对偏低。  
3) 在不同领域、场景下,命名实体的外延有差异,存在分类模糊的问题。不同命名实体之间界限不清晰,人名也经常出现在地名和组织名称中,存在大量的交叉和互相包含现象,而且部分命名实体常常容易与普通词混淆,影响识别效率。在个体户等商户中,组织名称中也存在大量的人名、地名、数字的现象,要正确标注这些命名实体类型,常常要涉及上下文语义层面的分析,这些都给命名实体的识别带来困难。  
4) 在不同的文化、领域、背景下,命名实体的外延有差异。对命名实体的定界和类型确定,目前还没有形成共同遵循的严格的命名规范。  
  
### 主要技术方法  
#### 基于规则和词典的方法  
基于规则的方法多采用语言学专家手工构造规则模板,选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词(如尾字)、中心词等方法, 以模式和字符串相匹配为主要手段,这类系统大多依赖于知识库和词典的建立。  
  
#### 基于统计的方法  
基于统计的方法利用人工标注的语料进行训练,标注语料时不需要广博的语言学知识,并且可以在较短时间内完成。这类系统在移植到新的领域时可以不做或少做改动,只要利用新语料进行一次训练即可。基于统计机器学习的方法主要包括:隐马尔可夫模型(Hidden Markov Model,HMM)、最大熵(Maximum Entropy,ME)、支持向量机(Support Vector Machine,SVM)、条件随机场(Conditional Random Fields,CRF)等。  

#### 基于BiLSTM-CRF的方法  
随着深度学习的在nlp领域的不断深入,许多nlp任务通过深度学习方法都取得了很大的突破,命名实体识别同样也可以通过深度学习方法来进行.基于统计的方法需要人为去定义很多的特征,最后提取的特征的维度会非常大,数据也比较稀疏.因此我们通过BiLSTM网络进行特征的提取,可以将维度控制在几百维度,对特征的提取也会更加全面.  
  
### BiLSTM-CRF  
BiLSTM-CRF的结构如下图所示:  
![](http://pbcgmnu5b.bkt.clouddn.com/pic1.png)  
  
#### 数据预处理  
数据预处理是将输入的数据转换成one-hot向量作为输入,我们的原始数据如下:  
![](http://pbcgmnu5b.bkt.clouddn.com/train_data.png)  
主要代码如下:  


```python
#分类标签的划分
tag2label = {"O": 0,
             "B-PER": 1, "I-PER": 2,
             "B-LOC": 3, "I-LOC": 4,
             "B-ORG": 5, "I-ORG": 6  
             }                

#建立词汇表
def vocab_build(vocab_path, corpus_path, min_count):
    """

    :param vocab_path:  词表的路径
    :param corpus_path: 语料的路径
    :param min_count:   最小的统计频数
    :return:
    """
    data = read_corpus(corpus_path)
    word2id = {}
    for sent_, tag_ in data:
        for word in sent_:
            if word.isdigit():                #将数字都转成<NUM>
                word = '<NUM>'
            elif ('\u0041' <= word <='\u005a') or ('\u0061' <= word <='\u007a'):  #将所有英文都转成<ENG>
                word = '<ENG>'
            if word not in word2id:
                word2id[word] = [len(word2id)+1, 1]
            else:
                word2id[word][1] += 1
    low_freq_words = []
    for word, [word_id, word_freq] in word2id.items():                      
        if word_freq < min_count and word != '<NUM>' and word != '<ENG>':      #统计出那些小于最小频数的字
            low_freq_words.append(word)
    for word in low_freq_words:
        del word2id[word]

    new_id = 1
    for word in word2id.keys():    #将每个字都对应唯一的序号
        word2id[word] = new_id
        new_id += 1
    word2id['<UNK>'] = new_id
    word2id['<PAD>'] = 0

    print(len(word2id))
    with open(vocab_path, 'wb') as fw:      #将统计好的词汇写入文档
        pickle.dump(word2id, fw)

#将每个句子都pad成同样的长度,长度为所有句子中的最大长度
def pad_sequences(sequences, pad_mark=0):
    """

    :param sequences:
    :param pad_mark:
    :return:
    """
    max_len = max(map(lambda x : len(x), sequences))
    seq_list, seq_len_list = [], []
    for seq in sequences:
        seq = list(seq)
        seq_ = seq[:max_len] + [pad_mark] * max(max_len - len(seq), 0)
        seq_list.append(seq_)
        seq_len_list.append(min(len(seq), max_len))
    return seq_list, seq_len_list
```

#### 定义Model  
该Model定义了七个计算图,我们列出主要的代码进行说明:


```python
#构建计算图
def build_graph(self):
    self.add_placeholders()   #添加占位符
    self.lookup_layer_op()    #转换成词向量
    self.biLSTM_layer_op()    #双向LSTM网络
    self.softmax_pred_op()    #预测网络
    self.loss_op()            #代价函数
    self.trainstep_op()       #优化代价函数
    self.init_op()            #初始化网络

#转换成词向量
def lookup_layer_op(self):
    with tf.variable_scope("words"):
        _word_embeddings = tf.Variable(self.embeddings,
                                       dtype=tf.float32,
                                       trainable=self.update_embedding,
                                       name="_word_embeddings")
        word_embeddings = tf.nn.embedding_lookup(params=_word_embeddings,
                                                 ids=self.word_ids,
                                                 name="word_embeddings")
    self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout_pl)  #将输入进行dropout处理

#双向LSTM网络
def biLSTM_layer_op(self):
    with tf.variable_scope("bi-lstm"):
        cell_fw = LSTMCell(self.hidden_dim) #前向的RNN核
        cell_bw = LSTMCell(self.hidden_dim) #后向的RNN核
        (output_fw_seq, output_bw_seq), _ = tf.nn.bidirectional_dynamic_rnn(
            cell_fw=cell_fw,
            cell_bw=cell_bw,
            inputs=self.word_embeddings,
            sequence_length=self.sequence_lengths,
            dtype=tf.float32)  #双向LSTM的输出结果
        output = tf.concat([output_fw_seq, output_bw_seq], axis=-1) #将输出的结果进行连接
        output = tf.nn.dropout(output, self.dropout_pl)  #将新的结果进行dropout处理

#解码的过程  通过BiLSTM得到的向量是包含上下文的信息,所以要通过一个解码的过程,所以用下面的全连接层就可以啦
    with tf.variable_scope("proj"):
        W = tf.get_variable(name="W",
                            shape=[2 * self.hidden_dim, self.num_tags],
                            initializer=tf.contrib.layers.xavier_initializer(),
                            dtype=tf.float32)

        b = tf.get_variable(name="b",
                            shape=[self.num_tags],
                            initializer=tf.zeros_initializer(),
                            dtype=tf.float32)

        s = tf.shape(output)
        output = tf.reshape(output, [-1, 2*self.hidden_dim])
        pred = tf.matmul(output, W) + b

        self.logits = tf.reshape(pred, [-1, s[1], self.num_tags])


#loss层 采用交叉熵函数
def loss_op(self):
    if self.CRF:
        log_likelihood, self.transition_params = crf_log_likelihood(inputs=self.logits,
                                                               tag_indices=self.labels,
                                                               sequence_lengths=self.sequence_lengths)
        self.loss = -tf.reduce_mean(log_likelihood)

    else:
        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits,
                                                                labels=self.labels)
        mask = tf.sequence_mask(self.sequence_lengths)
        losses = tf.boolean_mask(losses, mask)
        self.loss = tf.reduce_mean(losses)

def softmax_pred_op(self):
    if not self.CRF:
        self.labels_softmax_ = tf.argmax(self.logits, axis=-1)
        self.labels_softmax_ = tf.cast(self.labels_softmax_, tf.int32)
```

>CRF本身是一个非常复杂的算法,还好tensorflow中有相应的函数,可以一句代码就可以调用.最后再通过softmax将得分概率分布计算出来.  



```python
#优化代价函数
def trainstep_op(self):
    with tf.variable_scope("train_step"):
        self.global_step = tf.Variable(0, name="global_step", trainable=False)
        if self.optimizer == 'Adam':
            optim = tf.train.AdamOptimizer(learning_rate=self.lr_pl)
        elif self.optimizer == 'Adadelta':
            optim = tf.train.AdadeltaOptimizer(learning_rate=self.lr_pl)
        elif self.optimizer == 'Adagrad':
            optim = tf.train.AdagradOptimizer(learning_rate=self.lr_pl)
        elif self.optimizer == 'RMSProp':
            optim = tf.train.RMSPropOptimizer(learning_rate=self.lr_pl)
        elif self.optimizer == 'Momentum':
            optim = tf.train.MomentumOptimizer(learning_rate=self.lr_pl, momentum=0.9)
        elif self.optimizer == 'SGD':
            optim = tf.train.GradientDescentOptimizer(learning_rate=self.lr_pl)
        else:
            optim = tf.train.GradientDescentOptimizer(learning_rate=self.lr_pl)

        grads_and_vars = optim.compute_gradients(self.loss)
        grads_and_vars_clip = [[tf.clip_by_value(g, -self.clip_grad, self.clip_grad), v] for g, v in grads_and_vars]
        self.train_op = optim.apply_gradients(grads_and_vars_clip, global_step=self.global_step)
```

>这段代码主要是优化代价函数,方便进行参数的调整.

### 结果  
模型训练后的结果如下图:  

![](http://pbcgmnu5b.bkt.clouddn.com/train.png)
测试集的结果如下图:  

![](http://pbcgmnu5b.bkt.clouddn.com/test.png)  

调用训练后的模型进行句子检测结果如下图:  

![](http://pbcgmnu5b.bkt.clouddn.com/demo.png)

### 总结  
其实最后的结果相对来说还算满意,通过BiLSTM-CRF来实现命名实体识别,可以省去繁琐的特征工程构造,而且这个结果还要很大的提升空间,可以增大训练的数据集,还可以添加attention机制来进一步提升等.总之通过深度学习方法来进行命名实体识别的大致流程都是一样的,通过神经网络结构(除了BILSTM,像CNN,GRU等都是可以的)来进行特征的提取,最后都是加一层CRF,调好参数就差不多能达到很好的效果.
